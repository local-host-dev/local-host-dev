# 1) Setup
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, classification_report, confusion_matrix
import joblib

# If you created a Data asset, browse the left "Data" panel in notebooks, right-click -> "Mount" or copy path.
# Otherwise, put your file path here:
csv_path = "bank_churn.csv"  # replace with your mounted path or uploaded filename

df = pd.read_csv(csv_path)

# 2) Drop non-predictive IDs; set X,y
y = df["Exited"]
X = df.drop(columns=[c for c in ["Exited","CustomerId","Customer ID","RowNumber","Surname"] if c in df.columns])

# 3) Identify columns
numeric = X.select_dtypes(include=["int64","float64"]).columns.tolist()
categorical = X.select_dtypes(include=["object","bool"]).columns.tolist()

# 4) Preprocess
preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical)
    ]
)

# 5) Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 6) Two candidate models (you can add XGBoost if installed)
logreg = Pipeline(steps=[("prep", preprocess),
                        ("clf", LogisticRegression(max_iter=1000, class_weight="balanced", n_jobs=None))])

rf = Pipeline(steps=[("prep", preprocess),
                    ("clf", RandomForestClassifier(n_estimators=400, random_state=42, class_weight="balanced"))])

models = {"logreg": logreg, "rf": rf}
metrics = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_test)[:,1]
    pred = (proba >= 0.5).astype(int)
    acc = accuracy_score(y_test, pred)
    prec, rec, f1, _ = precision_recall_fscore_support(y_test, pred, average="binary")
    auc = roc_auc_score(y_test, proba)
    print(f"\n=== {name} ===")
    print(f"Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, AUC={auc:.4f}")
    print(classification_report(y_test, pred))
    print("Confusion matrix:\n", confusion_matrix(y_test, pred))
    metrics[name] = {"accuracy":acc,"precision":prec,"recall":rec,"f1":f1,"auc":auc}

# 7) Pick best by AUC
best_name = max(metrics, key=lambda k: metrics[k]["auc"])
best_model = models[best_name]
print("Best model:", best_name, metrics[best_name])

# 8) Save locally (outputs/) so AML captures it in the run artifacts
import os
os.makedirs("outputs", exist_ok=True)
joblib.dump(best_model, "outputs/model.pkl")
